# ② RAG实践教程

## 什么是RAG?

Retrieval-Augmented Generation（RAG）是一种人工智能框架，旨在通过将大型语言模型（Large Language Models，简称LLMs）与外部知识源相结合，提高由LLMs生成的响应的质量。这种方法的核心思想是，通过为模型提供额外的、最新的、可验证的信息，来补充LLM内部对信息的表示，从而使得生成的响应更加准确和可靠。

## RAG的工作原理

RAG框架包含两个主要阶段：检索（Retrieval）和内容生成（Generation）。

- 检索阶段：在这一阶段，算法会根据用户的提示或问题，搜索并检索与之相关的信息片段。在开放领域（open-domain）的消费级场景中，这些事实可能来自互联网上索引的文档；而在封闭领域（closed-domain）、企业级场景中，为了增加安全性和可靠性，通常会使用更有限的数据源集合。

- 内容生成阶段：检索到的外部知识被附加到用户的提示中，并传递给语言模型。LLM会根据这个增强后的提示以及其内部对训练数据的表示，合成一个针对用户即时需求的、引人入胜的答案。这个答案随后可以传递给聊天机器人，并附上其信息来源的链接。

## RAG的优势

- 确保最新、可靠的事实：通过RAG，模型能够访问到最新的、权威的事实信息，确保生成的响应内容是准确和可靠的。

- 用户可验证性：RAG允许用户访问模型的信息来源，这意味着模型的声明可以被检查以确保准确性，从而增加了用户对生成内容的信任。

- 降低成本：RAG是降低需要不断重新训练和更新LLMs成本的有效工具。它依赖于向量（数学数据表示形式）中包含的相关信息来丰富提示，这对于推荐引擎和聊天机器人等应用来说是非常有价值的。

## 目前RAG研究的范式

### Naive RAG

    Naive RAG 遵循传统的过程，包括索引、检索和生成，这也被描述为“Retrieve-Read”框架。索引从以 PDF、HTML、Word 和 Markdown 等不同格式清理和提取原始数据开始，然后将其转换为统一的纯文本格式。为了适应语言模型的上下文限制，文本被分割成更小、可消化的块。然后使用嵌入模型将块编码为向量表示并存储在向量数据库中。此步骤对于在后续检索阶段实现有效的相似性搜索至关重要。检索。在接收到用户查询后，RAG系统采用索引阶段中使用的相同编码模型，将查询转换为向量表示。然后它计算查询向量与索引语料库中的块向量之间的相似度分数。该系统优先考虑并检索与查询相似度最大的前 K 个块。这些块随后被用作提示中的扩展上下文。一代。将提出的查询和所选文档合成为一个连贯的提示，其中大型语言模型的任务是制定响应。该模型的回答方法可能因特定于任务的标准而异，允许它要么利用其固有的参数知识，要么将其响应限制在提供的文档中包含的信息上。在正在进行的对话的情况下，任何现有的对话历史都可以集成到提示中，使模型能够有效地参与多轮对话交互。

    总的来说，Naive RAG是最初的RAG实现，它包括索引、检索和生成三个步骤。Naive RAG简单直接，但存在检索质量不高、生成结果可能不准确等问题。


### Advanced RAG

    Advanced RAG是针对Naive RAG的不足进行改进的版本。它在检索阶段引入了预检索和后检索处理，例如使用滑动窗口、分层检索等技术来提高检索效率；在生成阶段，使用信息压缩、重排序等方法来提升生成质量。

    Advanced RAG引入了特定的改进来克服Naive RAG的局限性。专注于提高检索质量，它采用预检索和后检索策略。为了解决索引问题，Advanced RAG 通过使用滑动窗口方法、细粒度分割和元数据的结合来改进其索引技术。此外，它在几个阶段结合了几种优化方法来简化检索过程。

    第一，检索前过程。在这个阶段，主要重点是优化索引结构和原始查询。优化索引的目标是提高被索引的内容的质量。这涉及策略：增强数据粒度、优化索引结构、添加元数据、对齐优化和混合检索。虽然查询优化的目标是使用户的原始问题更清晰、更适合检索任务。常用方法包括查询重写查询转换、查询扩展等技术。

    第二，检索后过程。一旦检索到相关上下文，将其与查询有效地集成是至关重要的。检索后过程的主要方法包括重新排序块和上下文压缩。重新排序检索到的信息以将最相关的内容重新定位到提示的边缘是关键策略。这个概念已在 LlamaIndex、LangChain和 HayStack等框架中实现。将所有相关文档直接输入llm会导致信息过载，用不相关的内容稀释对关键细节的关注。为了缓解这种情况，检索后的努力集中在选择基本信息，强调关键部分，缩短要处理的上下文。

### Modular RAG

    Modular RAG则是一种更加灵活和模块化的RAG架构。它不再局限于固定的检索-生成流程，而是将整个流程拆分成多个模块，例如检索模块、生成模块、重排序模块等。这些模块可以独立开发、替换或组合，以满足不同任务的需求。Modular RAG还支持端到端的训练，使得各个模块之间可以协同优化。

    (1) 新模块：模块化 RAG 框架引入了额外的专用组件来增强检索和处理能力。搜索模块适应特定的场景，可以使用LLM生成的代码和查询语言在各种数据源(如搜索引擎、数据库和知识图)之间进行直接搜索。RAGFusion通过使用多查询策略来解决传统的搜索限制，该策略将用户查询扩展到不同的视角，利用并行向量搜索和智能重新排序来揭示显式和变革性的知识。内存模块利用LLM的内存来指导检索，创建一个无界内存池通过迭代自增强将文本与数据分布更接近。RAG系统中的路由通过不同的数据源导航，为查询选择最佳路径，无论是涉及摘要、特定的数据库搜索还是合并不同的信息流。Predict模块旨在通过直接通过LLM生成上下文来减少冗余和噪声，确保相关性和准确性。最后，任务适配器模块针对各种下游任务定制RAG，对零镜头输入进行快速检索，并通过少镜头查询生成创建特定于任务的检索器。这种综合方法不仅简化了检索过程，而且显着提高了检索到的信息的质量和相关性，满足了广泛的任务和查询，提高了精度和灵活性。

    （2）新模式:模块化RAG通过允许模块替换或重新配置来解决特定的挑战，提供了显著的适应性。这超出了 Naive 和 Advanced RAG 的固定结构，其特点是简单的“Retrieve”和“Read”机制。此外，模块化RAG通过整合新模块或调整现有模块之间的交互流来扩展这种灵活性，增强了它在不同任务中的适用性Rewrite-Retrieve-Read模型等创新利用LLM的能力通过重写模块和 LM 反馈机制来改进检索查询以更新重写模型，提高任务性能。同样，Generate-Read等方法将传统检索替换为 LLM 生成的内容，而ReciteRead强调从模型权重中检索，增强了模型处理知识密集型任务的能力。混合检索策略集成了关键字、语义和向量搜索，以满足不同的查询。此外，使用子查询和假设文档嵌入 (HyDE)旨在通过关注生成答案和真实文档之间的嵌入相似性来提高检索相关性。模块排列和交互调整，如演示搜索预测(DSP)框架和迭代检索-读取-检索-读取ITERRETGEN流程，展示了模块输出的动态使用，以支持另一个模块的功能，说明了对增强模块协同的复杂理解。模块化RAG Flow 的灵活编排展示了通过 FLARE 和 Self-RAG等技术的自适应检索的好处。这种方法通过评估基于不同场景的检索的必要性，超越了固定的 RAG 检索过程。灵活架构的另一个好处是RAG系统可以更容易地与其他技术(如微调或强化学习)集成。例如，这可能涉及微调检索器以获得更好的检索结果，微调生成器以获得更个性化的输出，或参与协作微调。