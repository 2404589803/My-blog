# Transformer模型源码分析

 **类型**：pytorch

在现阶段，Transformer 既可被视为一种架构,也可被视为一种模型。

## 架构

Transformer 最初被提出作为一种全新的序列到序列(Seq2Seq)模型架构,用于解决机器翻译等任务。它基于注意力(Attention)机制,摒弃了RNN和CNN等传统架构,完全使用注意力层在编码器-解码器结构中捕获输入和输出序列之间的长程依赖关系。

从这个角度来看,Transformer 提供了一种新颖的网络架构模板,可用于构建序列数据的编码器-解码器模型。研究人员可以在此基础上,针对不同任务调整结构细节、层数、注意力头数量等超参数,并使用不同的训练目标和数据集训练自己的 Transformer 模型。

## 模型

另一方面,在 NLP 领域里"Transformer 模型"往往特指原作者在论文中训练的那个用于机器翻译的具体模型实例。这个模型采用了 Transformer 架构,在大规模并行语料库上进行训练,取得了当时最佳的翻译质量。

由于其优秀表现,Transformer 模型本身也成为了一个重要的基准和研究对象。随后的 BERT、GPT 等大型预训练语言模型都是在 Transformer 模型的基础上发展而来的。

可以说，Transformer现阶段大模型的最底层基石。

现在，我们开始以代码的方式来逐一讲解复现Transformer，要特别注意的是，本代码的Transformer复现为**pytorch**版本,使用的是哈佛团队复现的代码，原项目的代码是在jupyter notebook中进行编写的。

原项目笔记本地址：

## Transformer复现

### Transformer整体模型架构图

整体的模型架构图如下：

![alt text](Transformer整体模型架构图.png)

### 预设

在pytorch版本的Transformer代码中，要预先导入以下的库：

```python



```

### Model Architecture  模型架构

#### Encoder and Decoder Stacks 编码器和解码器堆栈

大多数具有竞争力的神经序列转换模型都具有编码器-解码器结构。在本篇文章中，编码器将一个输入符号表示序列 $(x_1, \ldots, x_n)$ 映射到一系列连续表示 $\mathbf{z} = (z_1, \ldots, z_n)$。给定 $\mathbf{z}$，解码器然后生成一个输出符号序列 $(y_1\ldots,y_m)$，一次生成一个元素。在每一步，模型都是自回归，在生成下一个符号时，将先前生成的符号作为额外的输入。
